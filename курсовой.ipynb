{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347b8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396def62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9478129",
   "metadata": {},
   "source": [
    "# загрузка датасета\n",
    "https://www.kaggle.com/hgultekin/bbcnewsarchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11390f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>397.txt</td>\n",
       "      <td>BT program to beat dialler scams</td>\n",
       "      <td>BT is introducing two initiatives to help bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tech</td>\n",
       "      <td>398.txt</td>\n",
       "      <td>Spam e-mails tempt net shoppers</td>\n",
       "      <td>Computer users across the world continue to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>399.txt</td>\n",
       "      <td>Be careful how you code</td>\n",
       "      <td>A new European directive could put software w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>The man making sure US computer networks are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>401.txt</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>Online role playing games are time-consuming,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category filename                              title  \\\n",
       "0     business  001.txt  Ad sales boost Time Warner profit   \n",
       "1     business  002.txt   Dollar gains on Greenspan speech   \n",
       "2     business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3     business  004.txt  High fuel prices hit BA's profits   \n",
       "4     business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "...        ...      ...                                ...   \n",
       "2220      tech  397.txt   BT program to beat dialler scams   \n",
       "2221      tech  398.txt    Spam e-mails tempt net shoppers   \n",
       "2222      tech  399.txt            Be careful how you code   \n",
       "2223      tech  400.txt    US cyber security chief resigns   \n",
       "2224      tech  401.txt   Losing yourself in online gaming   \n",
       "\n",
       "                                                content  \n",
       "0      Quarterly profits at US media giant TimeWarne...  \n",
       "1      The dollar has hit its highest level against ...  \n",
       "2      The owners of embattled Russian oil giant Yuk...  \n",
       "3      British Airways has blamed high fuel prices f...  \n",
       "4      Shares in UK drinks and food firm Allied Dome...  \n",
       "...                                                 ...  \n",
       "2220   BT is introducing two initiatives to help bea...  \n",
       "2221   Computer users across the world continue to i...  \n",
       "2222   A new European directive could put software w...  \n",
       "2223   The man making sure US computer networks are ...  \n",
       "2224   Online role playing games are time-consuming,...  \n",
       "\n",
       "[2225 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_load = pd.read_csv('bbc-news-data.csv', sep='\\t')\n",
    "data_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5c8ed",
   "metadata": {},
   "source": [
    "# кол-во классов целевой переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b60ce61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_team = len(data_load['category'].unique())\n",
    "number_team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6134ca21",
   "metadata": {},
   "source": [
    "# функция очистки данных(входит в предобработку)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f8ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "#     text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa27a0a",
   "metadata": {},
   "source": [
    "# функция предобработки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eec9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data_load, column, test_size=0.2):\n",
    "    data = data_load.copy()\n",
    "    data.replace({'category':{'business':0, 'entertainment':1, 'politics':2, 'sport':3, 'tech':4}}, inplace=True)\n",
    "    target = data['category']\n",
    "    data = data[column]\n",
    "    data = data.apply(lambda x: clean_text(x))\n",
    "    tfidf = TfidfVectorizer(max_features=10000, stop_words='english')\n",
    "    data = tfidf.fit_transform(data).toarray()\n",
    "    if test_size == 0:\n",
    "        return data, target\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=test_size, random_state=42)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7023047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = transform(data_load, 'title')\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad7c37",
   "metadata": {},
   "source": [
    "# функция тренировки модели(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92cd33e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train):\n",
    "    model = keras.Sequential([Dense(32, activation='sigmoid'),\n",
    "#                           Dense(16, activation='relu'),\n",
    "                          Dense(number_team, activation='softmax')])    \n",
    "    model.compile(loss='MeanSquaredError', metrics=['CategoricalAccuracy'], optimizer='Adam')\n",
    "    y_train_cat = keras.utils.to_categorical(y_train, number_team)\n",
    "    model.fit(x_train, y_train_cat, batch_size=5, epochs=25, verbose=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a6242",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b71f83e3",
   "metadata": {},
   "source": [
    "# тренировка только на столбце 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654ac761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9752808988764045\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, x_test, y_train, y_test = transform(data_load, 'content')\n",
    "model_content = train_model(x_train, y_train)\n",
    "y = model_content.predict(x_test)\n",
    "y_pred_content = np.argmax(y, axis=1)\n",
    "print(f'accuracy = {sum(y_pred_content == y_test) / len(y_pred_content)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca7307",
   "metadata": {},
   "source": [
    "# проверка модели на всех данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1fad3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9950561797752809\n"
     ]
    }
   ],
   "source": [
    "data, target = transform(data_load, 'content', 0)\n",
    "y = model_content.predict(data)\n",
    "y_pred_content = np.argmax(y, axis=1)\n",
    "print(f'accuracy = {sum(y_pred_content == target) / len(y_pred_content)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2e03f",
   "metadata": {},
   "source": [
    "# объеденим  'title' и 'content' и обучим модель снова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e11135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_load['title_content'] = data_load['title'] + ' ' + data_load['content']\n",
    "x_train, x_test, y_train, y_test = transform(data_load, 'title_content')\n",
    "\n",
    "model_dual = train_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52aeadd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9775280898876404\n"
     ]
    }
   ],
   "source": [
    "y = model_dual.predict(x_test)\n",
    "y_pred = np.argmax(y, axis=1)\n",
    "print(f'accuracy = {sum(y_pred == y_test) / len(y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3f646",
   "metadata": {},
   "source": [
    "# метрика не значительно выросла от использования объединения 'title' и 'content'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6dbb1",
   "metadata": {},
   "source": [
    "# посмотрим матрицу ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a0866e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[111,   1,   1,   0,   2],\n",
       "       [  0,  72,   0,   0,   0],\n",
       "       [  2,   0,  73,   0,   1],\n",
       "       [  1,   0,   0, 101,   0],\n",
       "       [  0,   2,   0,   0,  78]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea74cb",
   "metadata": {},
   "source": [
    "#  лематизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42599a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 3.6274592876434326 secs.\n"
     ]
    }
   ],
   "source": [
    "from pywsd.utils import lemmatize_sentence\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9937e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = lemmatize_sentence(data_load['content'][0])\n",
    "# text = ' '.join(x)\n",
    "# stopWords = set(stopwords.words('english'))\n",
    "# len(' '.join(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3bc371",
   "metadata": {},
   "source": [
    "## перепишем функцию очистки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d05f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_lem(data_load, column, test_size=0.2):\n",
    "    data = data_load.copy()\n",
    "    data.replace({'category':{'business':0, 'entertainment':1, 'politics':2, 'sport':3, 'tech':4}}, inplace=True)\n",
    "    target = data['category']\n",
    "    data = data[column]\n",
    "    data = data.apply(lambda x: clean_text(x))\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for string in range(len(data)):                         # приведение в нормальную форму\n",
    "        x = lemmatize_sentence(data[string])\n",
    "        list_words = []\n",
    "        for word in x:                                      # добавим удаление стоп слов \n",
    "            word = word.strip()\n",
    "            if not word in stopWords and len(word) > 2:\n",
    "                list_words.append(word)\n",
    "        data[string] = ' '.join(list_words)\n",
    "    tfidf = TfidfVectorizer(max_features=10000)\n",
    "    data = tfidf.fit_transform(data).toarray()\n",
    "    if test_size == 0:\n",
    "        return data, target\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=test_size, random_state=42)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c86c80c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# a = transform_lem(data_load[:3], 'content', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e54e2e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[0][0], len(a[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fedccf",
   "metadata": {},
   "source": [
    "# обучим ту же модель по лематизированным данным"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a419e6",
   "metadata": {},
   "source": [
    "## без объединения 'title' и 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e08202c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, x_test, y_train, y_test = transform_lem(data_load, 'content')\n",
    "model_content_lem = train_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8903d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9775280898876404\n"
     ]
    }
   ],
   "source": [
    "y = model_content_lem.predict(x_test)\n",
    "y_pred = np.argmax(y, axis=1)\n",
    "print(f'accuracy = {sum(y_pred == y_test) / len(y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08d52267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[113,   1,   1,   0,   0],\n",
       "       [  0,  72,   0,   0,   0],\n",
       "       [  3,   0,  71,   1,   1],\n",
       "       [  1,   0,   0, 101,   0],\n",
       "       [  0,   2,   0,   0,  78]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693a5da",
   "metadata": {},
   "source": [
    "## объединенные 'title' и 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d9956b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data_load['title_content'] = data_load['title'] + ' ' + data_load['content']\n",
    "x_train, x_test, y_train, y_test = transform_lem(data_load, 'title_content')\n",
    "model_dual_lem = train_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd36dbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9752808988764045\n"
     ]
    }
   ],
   "source": [
    "y = model_dual_lem.predict(x_test)\n",
    "y_pred = np.argmax(y, axis=1)\n",
    "print(f'accuracy = {sum(y_pred == y_test) / len(y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69ae5dc",
   "metadata": {},
   "source": [
    "## объединение заголовка с телом поста не приносит плодов, исключим эту функцию на конечную модель. лематизация приносит так же незначительное улучшение метрик. в финальной модели оставим лематизацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b57eb",
   "metadata": {},
   "source": [
    "### сохраняем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2518281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# наилучшая модель model_content_lem\n",
    "\n",
    "name_model = 'finaly_model_kurs.h5'\n",
    "\n",
    "model_content_lem.save(name_model)\n",
    "# model_content.save_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b6ffc",
   "metadata": {},
   "source": [
    "### загрузим и проверим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e2f6d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_load = load_model(name_model)\n",
    "\n",
    "# model_load.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc06d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9775280898876404\n",
      "Wall time: 67.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, x_test, y_train, y_test = transform_lem(data_load, 'content')\n",
    "y = model_load.predict(x_test)\n",
    "y_pred = np.argmax(y, axis=1)\n",
    "print(f'accuracy = {sum(y_pred == y_test) / len(y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
